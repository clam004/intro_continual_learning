# intro_continual_learning

<img src="https://www.pnas.org/content/pnas/114/13/3521/F1.medium.gif">

This is a tutorial to connect the mathematics and machine learning theory to practical implementations addressing the continual learning problem of artificial intelligence. We will learn this in python by examining and decontructing a method called [elactic weight colsolidation](https://www.pnas.org/content/114/13/3521)

There is a shortage of tutorials that aim to directly help the student connect the math to the code. This is especially true in artificial intelligence where so much is published everyday. While it is true that the average programmer can load a "out of the box" library in 10 minutes and be running the latest model for a common task in 15 minutes, this is very different from being able to understand the mathematical theory behind the advances, the practical implementation details needed for real world deployment and the ability to adapt and combine the fundamental concepts to new applications not yet imagined.

Credit/References:

1.[Overcoming catastrophic forgetting in neural networks](https://www.pnas.org/content/114/13/3521)

2. [shivamsaboo17](https://github.com/shivamsaboo17/Overcoming-Catastrophic-forgetting-in-Neural-Networks)
