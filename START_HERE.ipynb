{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### elastic weight colsolidation\n",
    "\n",
    "In the figure below, $\\theta^{*}$ are the weights (ie parameters, synaptic strengths) learned by the neural network (NN) to solve old task A, shown as a vector in vector space. ie if this neural network as 100 total weights then this is a 2D representation of $\\mathbb{R}^{100}$ space. The blue horizontal arrow shows an example of catastrophic forgetting, whereby $\\theta^{*}$ moves out of the region that allows the NN to perform well at task A (grey), and into the center of a region that allows the NN to perform well at task B (cream). The downward green arrow is the update to $\\theta^{*}$ regularized by L2 penalty $\\alpha (\\theta_{i} - \\theta_{A , i}^{*})^{2}$ that causes it to move toward the cream region irrespective of the shape of the grey region. The desired update vector is the red arrow that moves the NN weights into a region capable of performing well at both tasks A and B. \n",
    "\n",
    "How Elastic Weight Cosolidation Changes Learning New Weights $\\theta^{*}$\n",
    "<p align=\"center\">\n",
    "<img src=\"https://raw.githubusercontent.com/clam004/intro_continual_learning/main/files/F1.large.jpg\" height=500 width=500 >\n",
    "</p>\n",
    "\n",
    "EWC encourages movement of weights along the red path by modifying the loss function when re-training a NN that has already been trained to convergence using the loss function for task A, $L_{A}$, which has settled on weights $\\theta_{A}$. When re-training the NN on task B using $L_{B}$, we add a term which penalizes changes to weights that are both far from $\\theta_{A}$, ie $(\\theta_{i} - \\theta_{A , i}^{*})^{2}$, and also high in $F_{i}$ which encodes the shape of the grey region.\n",
    "\n",
    "$$L \\left(\\right. \\theta \\left.\\right) = L_{B} \\left(\\right. \\theta \\left.\\right) + \\underset{i}{\\sum} \\frac{\\lambda}{2} F_{i} \\left(\\theta_{i} - \\theta_{A , i}^{*}\\right)^{2} $$\n",
    "\n",
    "### But what is F? \n",
    "\n",
    "F is the Fisher information matrix. We want to use the diagonal components in Fisher Information Matrix to identify which parameters are more important to task A and apply higher weights to them (the direction of the short axis of grey oval). To learn B we should instead change those weights where F_i is small (long axis of grey oval).  \n",
    "\n",
    "In the EWC paper:\n",
    "\n",
    "\"we approximate the posterior as a Gaussian distribution with mean given by the parameters θ∗A and a diagonal precision given by the diagonal of the Fisher information matrix F. F has three key properties (20): (i) It is equivalent to the second derivative of the loss near a minimum, (ii) it can be computed from first-order derivatives alone and is thus easy to calculate even for large models, and (iii) it is guaranteed to be positive semidefinite. Note that this approach is similar to expectation propagation where each subtask is seen as a factor of the posterior (21). where LB(θ) is the loss for task B only, λ sets how important the old task is compared with the new one, and i labels each parameter.\n",
    "\n",
    "When moving to a third task, task C, EWC will try to keep the network parameters close to the learned parameters of both tasks A and B. This can be enforced either with two separate penalties or as one by noting that the sum of two quadratic penalties is itself a quadratic penalty.\"\n",
    "\n",
    "### Lets learn what F is in the following example\n",
    "\n",
    "This article gives a very good explaination of F in the context of EWC: [fisher-info-matrix](https://andrewliao11.github.io/blog/fisher-info-matrix/)\n",
    "\n",
    "To compute F_i, we sample the data from task A once and calculate the empirical Fisher Information Matrix. \n",
    "\n",
    "$$\n",
    "I_{\\theta_\\mathcal{A}^*} = \\frac{1}{N}  \\sum_{i=1}^{N} \\nabla_\\theta log \\ p(x_{\\mathcal{A}, i}|\\theta_\\mathcal{A}^*) \\nabla_\\theta log \\ p(x_{\\mathcal{A}, i}|\\theta_\\mathcal{A}^*)^T\n",
    "$$\n",
    "\n",
    "This is just to say that the above equation is how you calculate the below equation from the data. For each pair of parameters in $\\theta$, $\\theta_i$ and$\\theta_j$ The Fisher Information matrix at position ij is\n",
    "\n",
    "$$\n",
    "I(\\theta)_{ij} = E\\left[ \\left( \\frac{\\partial}{\\partial\\theta_i}\\log f(X;\\theta) \\right)\\left( \\frac{\\partial}{\\partial\\theta_j}\\log f(X;\\theta) \\right) \\mid \\theta\\right]\n",
    "$$\n",
    "\n",
    "If this equation is hard to understand, then the code should make it clearer, dont worry, we will match parts of the code to the equation above so it becomes more tangible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
